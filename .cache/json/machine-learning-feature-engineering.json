{
    "data": {
        "markdownRemark": {
            "html": "<h2>Feature Engineering</h2>\n<!-- The article goes here, in GitHub-flavored Markdown. Feel free to add YouTube videos, images, and CodePen/JSBin embeds  -->\n<p>Machine Learning works best with well formed data. Feature engineering describes certain techniques to make sure we’re working with the best possible representation of the data we collected. Following are two techniques of feature engineering: scaling and selection.</p>\n<h3>Feature Scaling</h3>\n<p>Let’s assume your data contains the weight and height of people. The raw numbers of these two features have a high difference (e.g. 80 kg and 180 cm, or 175 lbs vs 5.9 ft), which could influence the outcome of certain Machine Learning algorithm. This is especially the case for algorithms that use <a href=\"https://en.wikipedia.org/wiki/Euclidean_distance\">distance functions</a>.</p>\n<p>To fix this isse, we represent the raw numbers in a 0 to 1 range. We can achieve this using the formula: <code>(x - xMin) / (xMax - xMin)</code>.</p>\n<p>Using this formula, we need to pay special attention to outliers, as these can affect the outcome drastically by pushing up xMax and pushing down xMin. That’s why outliers are often eliminated prior to scaling. </p>\n<h3>Feature Selection</h3>\n<p>It’s all about identifying the subset of features that are responsible for the trends we observe in our data.</p>\n<p>Why should we care? <a href=\"https://en.wikipedia.org/wiki/Curse_of_dimensionality\">Curse of Dimensionality</a> is a big enemy in times of Big Data. We can’t use all of our tens to hundreds of features. This would not only raise the dimensionality of our data through the roof (2^n, where n is the number of features), but also often don’t make any sense in specific use cases. Imagine wanting to predict the weather of tomorrow: It will be more likely that the weather trend of last days is more important in this scenario than the babies born in the last days. So you could easily just eliminate the babies-feature.</p>\n<p>But forget babies for now, let’s dive into more detail.</p>\n<h4>Filtering &#x26; Wrapping</h4>\n<p>Here we describe two general approaches. Filtering methods act independently from your chosen learning algorithm &#x26; wrapping methods incorporate your learner.</p>\n<p>Filter methods select the subset of features before injecting the data into your ML algorithm. They use e.g. the correleation with the to-be-predicted variable to identify which subset of features to choose. These methods are relatively fast to compute, but don’t take advantage of the <a href=\"https://en.wikipedia.org/wiki/Inductive_bias\">bias of the learner</a> because filtering is happening independent of your chosen ML model.</p>\n<p>Wrapping search algorithms do take advantage of the learning bias, as they incorporate your chosen ML model. These methods function by removing the feature that has the lowest change in score when removed and repeating this process until the score changes significantly. This means running your learning algorithm over and over again, which can lead to significant computation times. These methods also have the danger of overfitting, as you’re basically optimizing the feature set based on your chosen ML model.</p>\n<h4>Relevance</h4>\n<p>Another way of selecting features is using the <a href=\"https://scholar.google.de/scholar?q=Bayes+Optimal+Classifier&#x26;hl=en&#x26;as_sdt=0&#x26;as_vis=1&#x26;oi=scholart&#x26;sa=X&#x26;ved=0ahUKEwiO16X0tIbXAhXiKsAKHbGrBzoQgQMIJjAA\">BOC (Bayes Optimal Classifier)</a>. The rule of thumbs here are:</p>\n<ul>\n<li>a feature is strongly relevant if removing it degrades the BOC</li>\n<li>a feature is weakly relevant if it is not strongly relevant &#x26; adding it in combination with other features improves the BOC</li>\n<li>otherwise a feature is irrelevant</li>\n</ul>\n<p>Well, not always. It depends on the amount of data you have and the strength of competing signals. You can help your algorithm “focus” on what’s important by highlighting it beforehand.</p>\n<ul>\n<li>Indicator variable from thresholds: Let’s say you’re studying alcohol preferences by U.S. consumers and your dataset has an age feature. You can create an indicator variable for age >= 21 to distinguish subjects who were over the legal drinking age.</li>\n<li>Indicator variable from multiple features: You’re predicting real-estate prices and you have the features n<em>bedrooms and n</em>bathrooms. If houses with 2 beds and 2 baths command a premium as rental properties, you can create an indicator variable to flag them.</li>\n<li>Indicator variable for special events: You’re modeling weekly sales for an e-commerce site. You can create two indicator variables for the weeks of Black Friday and Christmas.</li>\n<li>Indicator variable for groups of classes: You’re analyzing website conversions and your dataset has the categorical feature traffic<em>source. You could create an indicator variable for paid</em>traffic by flagging observations with traffic source values of  “Facebook Ads” or “Google Adwords”.</li>\n</ul>\n<h2>Interaction Features</h2>\n<p>The next type of feature engineering involves highlighting interactions between two or more features.</p>\n<p>Have you ever heard the phrase, “the sum is greater than the parts?” Well, some features can be combined to provide more information than they would as individuals.</p>\n<p>Specifically, look for opportunities to take the sum, difference, product, or quotient of multiple features.</p>\n<p>*Note: We don’t recommend using an automated loop to create interactions for all your features. This leads to “feature explosion.”</p>\n<ul>\n<li>Sum of two features: Let’s say you wish to predict revenue based on preliminary sales data. You have the features sales<em>blue</em>pens and sales<em>black</em>pens. You could sum those features if you only care about overall sales_pens.</li>\n<li>Difference between two features: You have the features house<em>built</em>date and house<em>purchase</em>date. You can take their difference to create the feature house<em>age</em>at_purchase.</li>\n<li>Product of two features: You’re running a pricing test, and you have the feature price and an indicator variable conversion. You can take their product to create the feature earnings.</li>\n<li>Quotient of two features: You have a dataset of marketing campaigns with the features n<em>clicks and n</em>impressions. You can divide clicks by impressions to create  click<em>through</em>rate, allowing you to compare across campaigns of different volume.</li>\n</ul>\n<h4>More Information:</h4>\n<!-- Please add any articles you think might be helpful to read before writing the article -->\n<ul>\n<li><a href=\"https://pdfs.semanticscholar.org/6e51/8946c59c8c5d005054af319783b3eba128a9.pdf\">Paper exploring “Feature Engineering for Text Classification”</a></li>\n<li><a href=\"https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/\">Article “Discover Feature Engineering, How to Engineer Features and How to Get Good at It”</a></li>\n</ul>",
            "fields": {
                "slug": "/machine-learning/feature-engineering/"
            },
            "frontmatter": {
                "title": "Feature Engineering"
            }
        }
    },
    "pathContext": {
        "slug": "/machine-learning/feature-engineering/"
    }
}